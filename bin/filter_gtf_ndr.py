#! /usr/bin/env python3
from typing import Dict, Set, Tuple
from collections import namedtuple

from GTF import GTF

TranscriptProb = namedtuple("TranscriptProb", ["gene_id", "tx_id", "ndr"])


def parse_bambu(line) -> TranscriptProb:
    return TranscriptProb(line[1], line[0].lower(), float(line[2]))


def parse_tfkmers(line) -> Tuple[TranscriptProb, str]:
    ids = line[0].split("::")
    return TranscriptProb(ids[0], ids[1].lower(), float(line[1])), ids[2]


StrandRecord = namedtuple("StrandRecord", ["ndr", "strand"])


def parse_ndr(csv, origin, th) -> Tuple[Set[str], Dict[str, StrandRecord]]:
    s = set()
    strand_dict = dict()

    # Skip header
    next(csv)

    strand = None
    for line in csv:
        line = line.split(",")

        if origin == "bambu":
            tx_prob = parse_bambu(line)
        elif origin == "tfkmers":
            tx_prob, strand = parse_tfkmers(line)
        else:
            exit("Unknown method")

        if tx_prob.ndr < th:
            s.add(tx_prob.tx_id)

            # Extract strand from sequence name to restrand GTF records
            if origin == "tfkmers":
                # If both extremities are tested, keep only lower extremity prob
                if (
                    tx_prob.tx_id not in strand_dict
                    or tx_prob.ndr < strand_dict[tx_prob.tx_id].ndr
                ):
                    strand_dict[tx_prob.tx_id] = StrandRecord(tx_prob.ndr, strand)

    return s, strand_dict


def filter_count_matrix(file, transcripts, wr):
    print(next(file), file=wr)
    for line in file:
        line_splitted = line.split("\t")
        if (
            line_splitted[0].startswith("BambuTx")
            and line_splitted[0].lower() not in transcripts
        ):
            continue
        print(line.rstrip(), file=wr)


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(
        description="Filter your extended GTF generated by ANNEXA. See https://github.com/igdrion/annexa for more informations"
    )

    # Required args
    parser.add_argument(
        "--gtf",
        help="Path to extended_annotations.gtf generated by ANNEXA",
        type=argparse.FileType("r"),
        required=True,
    )
    parser.add_argument(
        "--counts_tx",
        help="Path to counts_transcript.txt generated by bambu",
        type=argparse.FileType("r"),
        required=True,
    )
    parser.add_argument(
        "--bambu",
        help="Path to NDR generated by ANNEXA/bambu",
        type=argparse.FileType("r"),
        required=True,
    )
    parser.add_argument(
        "--bambu-threshold",
        help="Threshold to bambu NDR",
        type=float,
        default=0.2,
    )
    parser.add_argument(
        "--tfkmers",
        help="Path to output.csv generated by ANNEXA/transforkmers",
        type=argparse.FileType("r"),
        required=True,
    )
    parser.add_argument(
        "--tfkmers-threshold",
        help="Threshold to transforkmers NDR",
        type=float,
        default=0.2,
    )

    parser.add_argument(
        "--operation",
        help="Operation performed. Choices: union, intersection",
        type=str,
        default="intersection",
        choices=["union", "intersection"],
    )
    args = parser.parse_args()

    ###################################################
    filter_bambu, _ = parse_ndr(args.bambu, "bambu", args.bambu_threshold)
    filter_tfkmers, strand_dict = parse_ndr(
        args.tfkmers, "tfkmers", args.tfkmers_threshold
    )

    if args.operation == "union":
        filter = filter_bambu | filter_tfkmers
    else:
        filter = filter_bambu & filter_tfkmers

    with open("unformat.novel.filter.gtf", "w") as wr:
        for record in GTF.parse_by_line(args.gtf):
            if "transcript_id" in record:
                tx_id = record["transcript_id"].lower()

                if tx_id in filter:
                    # If operation == "union", tx_id can be OK in bambu
                    # but not in TFKmers. So strand not defined
                    if tx_id in strand_dict:
                        record.strand = strand_dict[tx_id].strand

                    print(record, file=wr)

    with open("counts_transcript.filter.txt", "w") as wr:
        filter_count_matrix(args.counts_tx, filter, wr)
